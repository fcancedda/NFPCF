{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import torch\n",
    "from random import choice\n",
    "import math\n",
    "from torch import LongTensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def get_test_instances_with_random_samples(data, random_samples,num_items,device):\n",
    "    user_input = np.zeros((random_samples+1))\n",
    "    item_input = np.zeros((random_samples+1))\n",
    "\n",
    "    # positive instance\n",
    "    user_input[0] = data[0]\n",
    "    item_input[0] = data[1]\n",
    "    i = 1\n",
    "    # negative instances\n",
    "    checkList = data[1]\n",
    "    for t in range(random_samples):\n",
    "        j = np.random.randint(num_items)\n",
    "        while j == checkList:\n",
    "            j = np.random.randint(num_items)\n",
    "        user_input[i] = data[0]\n",
    "        item_input[i] = j\n",
    "        i += 1\n",
    "    return torch.LongTensor(user_input).to(device), torch.LongTensor(item_input).to(device)\n",
    "\n",
    "def get_instances_with_random_neg_samples(train,num_items, num_negatives,device):\n",
    "    user_input = np.zeros((len(train)+len(train)*num_negatives))\n",
    "    item_input = np.zeros((len(train)+len(train)*num_negatives))\n",
    "    labels = np.zeros((len(train)+len(train)*num_negatives))\n",
    "\n",
    "    neg_samples = choice(num_items, size=(10*len(train)*num_negatives,)) # multiply by 2 to make sure, we dont run out of negative samples\n",
    "    neg_counter = 0\n",
    "    i = 0\n",
    "    for n in range(len(train)):\n",
    "        # positive instance\n",
    "        user_input[i] = train['user_id'][n]\n",
    "        item_input[i] = train['like_id'][n]\n",
    "        labels[i] = 1\n",
    "        i += 1\n",
    "        # negative instances\n",
    "        checkList = list(train['like_id'][train['user_id']==train['user_id'][n]])\n",
    "        for t in range(num_negatives):\n",
    "            j = neg_samples[neg_counter]\n",
    "            while j in checkList:\n",
    "                neg_counter += 1\n",
    "                j = neg_samples[neg_counter]\n",
    "            user_input[i] = train['user_id'][n]\n",
    "            item_input[i] = j\n",
    "            labels[i] = 0\n",
    "            i += 1\n",
    "            neg_counter += 1\n",
    "    return torch.LongTensor(user_input).to(device), torch.LongTensor(item_input).to(device), torch.FloatTensor(labels).to(device)\n",
    "\n",
    "def evaluate_model(model,df_val,top_K,random_samples, num_items):\n",
    "    model.eval()\n",
    "    avg_HR = np.zeros((len(df_val),top_K))\n",
    "    avg_NDCG = np.zeros((len(df_val),top_K))\n",
    "\n",
    "    for i in range(len(df_val)):\n",
    "        test_user_input, test_item_input = get_test_instances_with_random_samples(df_val[i], random_samples,num_items,device)\n",
    "        y_hat = model(test_user_input, test_item_input)\n",
    "        y_hat = y_hat.cpu().detach().numpy().reshape((-1,))\n",
    "        test_item_input = test_item_input.cpu().detach().numpy().reshape((-1,))\n",
    "        map_item_score = {}\n",
    "        for j in range(len(y_hat)):\n",
    "            map_item_score[test_item_input[j]] = y_hat[j]\n",
    "        for k in range(top_K):\n",
    "            # Evaluate top rank list\n",
    "            ranklist = heapq.nlargest(k, map_item_score, key=map_item_score.get)\n",
    "            gtItem = test_item_input[0]\n",
    "            avg_HR[i,k] = getHitRatio(ranklist, gtItem)\n",
    "            avg_NDCG[i,k] = getNDCG(ranklist, gtItem)\n",
    "\n",
    "    avg_HR = np.mean(avg_HR, axis = 0)\n",
    "    avg_NDCG = np.mean(avg_NDCG, axis = 0)\n",
    "    return avg_HR, avg_NDCG\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"evaluations/nfcf/test_userPages.csv\")\n",
    "test_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% NFCF\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from evaluate import Evaluate\n",
    "from data import DataGenerator\n",
    "import torch.nn as nn\n",
    "class neuralCollabFilter(nn.Module):\n",
    "    def __init__(self, num_users, num_likes, embed_size, num_hidden, output_size):\n",
    "        super(neuralCollabFilter, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embed_size)\n",
    "        self.like_emb = nn.Embedding(num_likes, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size * 2, num_hidden[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(num_hidden[0], num_hidden[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(num_hidden[1], num_hidden[2])\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(num_hidden[2], num_hidden[3])\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.outLayer = nn.Linear(num_hidden[3], output_size)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.like_emb(v)\n",
    "        out = torch.cat([U, V], dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.outLayer(out)\n",
    "        out = self.out_act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "ncf = neuralCollabFilter(6040, 3416, 128, np.array([128, 64, 32, 16]), 1).to(device)\n",
    "ncf.load_state_dict(torch.load(\"evaluations/nfcf/preTrained_NCF\",  map_location=torch.device('cpu')))\n",
    "ncf.to(device)\n",
    "\n",
    "data = DataGenerator()\n",
    "evaluator = Evaluate(ncf, data, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "t1 = time()\n",
    "avg_HR_preTrain, avg_NDCG_preTrain = evaluate_model(ncf,data.test[['uid', 'mid']].values,15,100, 3416)\n",
    "t2 = time()\n",
    "\n",
    "t2-t1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avg_HR_preTrain\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "hr, ndcg = evaluator()\n",
    "t2 = time()\n",
    "t2 - t1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import time\n",
    "def rank(l, item):\n",
    "    # rank of the test item in the list of negative instances\n",
    "    # returns the number of elements that the test item is bigger than\n",
    "\n",
    "    index = 0\n",
    "    for element in l:\n",
    "        if element > item:\n",
    "            index += 1\n",
    "            return index\n",
    "        index += 1\n",
    "    return index\n",
    "def eval_model(model, data, num_users=6040):\n",
    "    # Evaluates the model and returns HR@10 and NDCG@10\n",
    "    hits = 0\n",
    "    ndcg = 0\n",
    "    for u in range(num_users):\n",
    "        user = data.testing_tensors[0][u].squeeze().to(device)\n",
    "        item = data.testing_tensors[1][u].squeeze().to(device)\n",
    "        y = model(user, item)\n",
    "\n",
    "        y = y.tolist()\n",
    "        y = sum(y, [])\n",
    "        first = y.pop(0)\n",
    "        y.sort()\n",
    "        ranking = rank(y, first)\n",
    "        if ranking > 90:\n",
    "            hits += 1\n",
    "            ndcg += np.log(2) / np.log(len(user) - ranking + 1)\n",
    "\n",
    "    hr = hits / num_users\n",
    "    ndcg = ndcg / num_users\n",
    "    return hr, ndcg\n",
    "\n",
    "t1 = time()\n",
    "hr2, ndcg2 = eval_model(ncf, data)\n",
    "t2 = time()\n",
    "\n",
    "print(t2-t1)\n",
    "print(hr2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from tf\n",
    "\n",
    "import math\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "class Dataset(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, path):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        # self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "\n",
    "        # self.num_users, self.num_items = self.trainMatrix.shape\n",
    "\n",
    "    @staticmethod\n",
    "    def load_rating_file_as_list(filename):\n",
    "        ratingList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                ratingList.append([user, item])\n",
    "                line = f.readline()\n",
    "        return ratingList\n",
    "\n",
    "    def load_negative_file(self, filename):\n",
    "        negativeList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                negatives = []\n",
    "                for x in arr[1: ]:\n",
    "                    negatives.append(int(x))\n",
    "                negativeList.append(negatives)\n",
    "                line = f.readline()\n",
    "        return negativeList\n",
    "\n",
    "    # def load_rating_file_as_matrix(self, filename):\n",
    "    #     '''\n",
    "    #     Read .rating file and Return dok matrix.\n",
    "    #     The first line of .rating file is: num_users\\t num_items\n",
    "    #     '''\n",
    "    #     # Get number of users and items\n",
    "    #     num_users, num_items = 0, 0\n",
    "    #     with open(filename, \"r\") as f:\n",
    "    #         line = f.readline()\n",
    "    #         while line != None and line != \"\":\n",
    "    #             arr = line.split(\"\\t\")\n",
    "    #             u, i = int(arr[0]), int(arr[1])\n",
    "    #             num_users = max(num_users, u)\n",
    "    #             num_items = max(num_items, i)\n",
    "    #             line = f.readline()\n",
    "    #     # Construct matrix\n",
    "    #     mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    #     with open(filename, \"r\") as f:\n",
    "    #         line = f.readline()\n",
    "    #         while line != None and line != \"\":\n",
    "    #             arr = line.split(\"\\t\")\n",
    "    #             user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "    #             if (rating > 0):\n",
    "    #                 mat[user, item] = 1.0\n",
    "    #             line = f.readline()\n",
    "    #     return mat\n",
    "\n",
    "dataset = Dataset('evaluations/tf/ml-1m')\n",
    "testRatings, testNegatives = dataset.testRatings, dataset.testNegatives\n",
    "\n",
    "# Global variables that are shared across processes\n",
    "_model = ncf\n",
    "_testRatings = testRatings\n",
    "_testNegatives = testNegatives\n",
    "_K = 10\n",
    "\n",
    "def tf_eval(num_thread):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    # _model = model\n",
    "    # _testRatings = testRatings\n",
    "    # _testNegatives = testNegatives\n",
    "    # _K = K\n",
    "\n",
    "    hits, ndcgs = [],[]\n",
    "    if num_thread > 1: # Multi-thread\n",
    "        pool = multiprocessing.Pool(processes=num_thread)\n",
    "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        hits = [r[0] for r in res]\n",
    "        ndcgs = [r[1] for r in res]\n",
    "        return (hits, ndcgs)\n",
    "    # Single thread\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    test_user_input, test_item_input = get_test_instances_with_random_samples(test_data.values[idx], 100,data.num_movies,device)\n",
    "    predictions = _model(test_user_input, test_item_input)\n",
    "    # predictions = _model(LongTensor(users), LongTensor(items))\n",
    "    map_item_score = dict(zip(items, predictions))\n",
    "\n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "(hits, ndcgs) = tf_eval(1)\n",
    "t2 = time()\n",
    "t2 - t1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'original {avg_HR_preTrain}\\n opt {hr2} \\ntorch{np.array(hits).mean()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_epochs = 25\n",
    "batch_size = 2048\n",
    "learning_rate = .001\n",
    "top_k = 10\n",
    "\n",
    "def train_ncf(model):\n",
    "    # data.get_train_instances(seed=e)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),  lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "    dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=0)\n",
    "    t1 = time()\n",
    "\n",
    "    it_per_epoch = len(data) / batch_size\n",
    "    for i in range(num_epochs):\n",
    "        model.train()\n",
    "        print(\"Starting epoch \", i + 1)\n",
    "        j = 0\n",
    "        for batch in dataloader:\n",
    "            u, m, r = batch\n",
    "            # move tensors to cuda\n",
    "            u = u.to(device)\n",
    "            m = m.to(device)\n",
    "            r = r.to(device)\n",
    "\n",
    "            y_hat = model(u.squeeze(1), m.squeeze(1))\n",
    "\n",
    "            loss = torch.nn.BCELoss()  # (weight=w, reduction=\"mean\")\n",
    "            loss = loss(y_hat, r.float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if j % int(1 + it_per_epoch / 10) == 0:\n",
    "                print(\"Progress: \", round(100 * j / it_per_epoch), \"%\")\n",
    "            j+=1\n",
    "\n",
    "        # Epoch metrics\n",
    "        t2 = time()\n",
    "        print(\"Epoch time:\", round(t2 - t1), \"seconds\")\n",
    "        print(\"Loss:\", loss / i)\n",
    "        ncf.eval()\n",
    "\n",
    "\n",
    "        print('baseline ')\n",
    "        t1 = time()\n",
    "        hr, ndcg = evaluate_model(ncf,data.test[['uid', 'mid']].values,10,100, 3416)\n",
    "        t2 = time()\n",
    "        print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        print(f\"HR@{top_k}:{hr[-1]}\")\n",
    "\n",
    "        print(\"(evaluator)...\")\n",
    "        t1 = time()\n",
    "        hr, ndcg = evaluator()\n",
    "        # hr, ndcg = evaluate(model, data.test, top_K)\n",
    "        t2 = time()\n",
    "        print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        print(f\"HR@{top_k}:{hr}\")\n",
    "\n",
    "        print(\"Evaluating (eval_model)...\")\n",
    "        t1 = time()\n",
    "        hr, ndcg = eval_model(model, data)\n",
    "        t2 = time()\n",
    "        print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        print(f\"HR@{top_k}:{hr}\")\n",
    "\n",
    "\n",
    "        print(\"Evaluating (tf)...\")\n",
    "        t1 = time()\n",
    "        hr, ndcg = tf_eval(1)\n",
    "        t2 = time()\n",
    "\n",
    "        print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        print(f\"HR@{top_k}:{np.array(hr).mean()}\")\n",
    "        # new\n",
    "        # HR, NDCG = evaluate_model(model, data, validation=False)\n",
    "        # updated\n",
    "        # hr, ndcg = evaluate_model(model, data.test, top_K, random_samples)\n",
    "        # original\n",
    "        loss = 0\n",
    "        print()\n",
    "\n",
    "    print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ncf(ncf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}