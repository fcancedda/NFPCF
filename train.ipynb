{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from models import NCF\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import DataGenerator\n",
    "\n",
    "data = DataGenerator()\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- PARSING DATA ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- LOAD DATA ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/kf/j1jq_1qs78n4gd1vczc8vfw80000gn/T/ipykernel_11583/1767240841.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mdef\u001B[0m \u001B[0mevaluate_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf_val\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop_K\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_samples\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mavg_HR\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf_val\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop_K\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "'''TRAIN MODEL ( MINI BATCH )'''\n",
    "\n",
    "def evaluate_model(model, df_val:pd.DataFrame, top_K, random_samples):\n",
    "    model.eval()\n",
    "    avg_HR = np.zeros((len(df_val), top_K))\n",
    "    avg_NDCG = np.zeros((len(df_val), top_K))\n",
    "    test_df = data.add_negatives(\n",
    "        df_val,\n",
    "        n_samples=random_samples\n",
    "    )\n",
    "    gp = test_df.groupby('uid')\n",
    "    for g in gp:\n",
    "        for k in range(top_K):\n",
    "            users, items = torch.LongTensor(g.uid).to(device), torch.LongTensor(g.mid).to(device)\n",
    "            y_hat = model(users, items)\n",
    "            y_hat = y_hat.cpu().detach().numpy().reshape((-1,))\n",
    "            test_item_input = items.cpu().detach().numpy().reshape((-1,))\n",
    "            map_item_score = dict(zip(test_item_input, y_hat))\n",
    "\n",
    "    for i in range(df_val.shape[0]):\n",
    "        for k in range(top_K):\n",
    "            test_df = data.add_negatives(\n",
    "                pd.DataFrame(data.test.iloc[i]).T,\n",
    "                n_samples=random_samples\n",
    "            )\n",
    "            users, items = torch.LongTensor(test_df.uid).to(device), torch.LongTensor(test_df.mid).to(device)\n",
    "            y_hat = model(users, items)\n",
    "            y_hat = y_hat.cpu().detach().numpy().reshape((-1,))\n",
    "            test_item_input = items.cpu().detach().numpy().reshape((-1,))\n",
    "            map_item_score = dict(zip(test_item_input, y_hat))\n",
    "            # Evaluate top rank list\n",
    "            ranklist = heapq.nlargest(k, map_item_score, key=map_item_score.get)\n",
    "            gtItem = test_item_input[0]\n",
    "            for item in ranklist:\n",
    "                if item==gtItem:\n",
    "                    avg_HR[i, k] = 1\n",
    "                    avg_NDCG[i, k] = math.log(2) / math.log(i+2)\n",
    "                else:\n",
    "                    avg_HR[i, k] = 0\n",
    "                    avg_NDCG[i, k] = 0\n",
    "    avg_HR = np.mean(avg_HR, axis=0)\n",
    "    avg_NDCG = np.mean(avg_NDCG, axis=0)\n",
    "    return avg_HR, avg_NDCG\n",
    "\n",
    "def evaluate(model, df_val:pd.DataFrame, k=10):\n",
    "    test_df = data.add_negatives(df_val, n_samples=random_samples)\n",
    "    users, items = torch.LongTensor(test_df.uid).to(device), torch.LongTensor(test_df.mid).to(device)\n",
    "    y_hat = model(users, items)\n",
    "    test_df['score'] = y_hat.detach.numpy().reshape((-1,))\n",
    "    grouped = test_df.copy(deep=True)\n",
    "    grouped['ranked'] = grouped.groupby('uid')['score'].rank(method='first', ascending=False)\n",
    "    grouped.sort_values(['uid', 'rank'], inplace=True)\n",
    "    top_k = grouped[grouped['rank']<=k]\n",
    "    test_in_top_k = top_k[top_k['rating'] == 1]\n",
    "    hr = test_in_top_k.shape[0] / data.num_users\n",
    "    test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: np.log(2)/np.log(1 + x))\n",
    "    ndcg = test_in_top_k.ndcg.sum() / data.num_users\n",
    "    return hr, ndcg\n",
    "\n",
    "def get_test_instances_with_random_samples(data, random_samples, num_items, device):\n",
    "    user_input = np.zeros((random_samples + 1))\n",
    "    item_input = np.zeros((random_samples + 1))\n",
    "\n",
    "    # positive instance\n",
    "    user_input[0] = data[0]\n",
    "    item_input[0] = data[1]\n",
    "    i = 1\n",
    "    # negative instances\n",
    "    checkList = data[1]\n",
    "    for t in range(random_samples):\n",
    "        j = np.random.randint(num_items)\n",
    "        while j == checkList:\n",
    "            j = np.random.randint(num_items)\n",
    "        user_input[i] = data[0]\n",
    "        item_input[i] = j\n",
    "        i += 1\n",
    "    return torch.LongTensor(user_input).to(device), torch.LongTensor(item_input).to(device)\n",
    "\n",
    "def evaluate_model(model, df_val, top_K, random_samples, num_items):\n",
    "    model.eval()\n",
    "    avg_HR = np.zeros((len(df_val), top_K))\n",
    "    avg_NDCG = np.zeros((len(df_val), top_K))\n",
    "\n",
    "    for i in range(len(df_val)):\n",
    "        test_user_input, test_item_input = get_test_instances_with_random_samples(df_val[i], random_samples, num_items,\n",
    "                                                                                  device)\n",
    "        y_hat = model(test_user_input, test_item_input)\n",
    "        y_hat = y_hat.cpu().detach().numpy().reshape((-1,))\n",
    "        test_item_input = test_item_input.cpu().detach().numpy().reshape((-1,))\n",
    "        map_item_score = {}\n",
    "        for j in range(len(y_hat)):\n",
    "            map_item_score[test_item_input[j]] = y_hat[j]\n",
    "        for k in range(top_K):\n",
    "            # Evaluate top rank list\n",
    "            ranklist = heapq.nlargest(k, map_item_score, key=map_item_score.get)\n",
    "            gtItem = test_item_input[0]\n",
    "            avg_HR[i, k] = getHitRatio(ranklist, gtItem)\n",
    "            avg_NDCG[i, k] = getNDCG(ranklist, gtItem)\n",
    "    avg_HR = np.mean(avg_HR, axis=0)\n",
    "    avg_NDCG = np.mean(avg_NDCG, axis=0)\n",
    "    return avg_HR, avg_NDCG\n",
    "\n",
    "num_epochs = 25\n",
    "learning_rate = .001\n",
    "\n",
    "emb_size = 128\n",
    "hidden_layers = np.array([emb_size, 64, 32, 16])\n",
    "output_size = 1\n",
    "random_samples = 100\n",
    "num_negatives = 4\n",
    "top_K = 10\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- Functions for evaluation ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    # data.get_train_instances(seed=e)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),  lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "    dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)\n",
    "    t1 = time.time()\n",
    "\n",
    "    it_per_epoch = len(data) / batch_size\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"Starting epoch \", i + 1)\n",
    "        j = 0\n",
    "        for batch in dataloader:\n",
    "            u, m, r = batch\n",
    "            # move tensors to cuda\n",
    "            u = u.to(device)\n",
    "            m = m.to(device)\n",
    "            r = r.to(device)\n",
    "\n",
    "            y_hat = model(u, m)\n",
    "\n",
    "            loss = torch.nn.BCELoss()  # (weight=w, reduction=\"mean\")\n",
    "\n",
    "            loss = loss(y_hat, r.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % int(1 + it_per_epoch / 10) == 0:\n",
    "                print(\"Progress: \", round(100 * j / it_per_epoch), \"%\")\n",
    "\n",
    "        # Epoch metrics\n",
    "        t2 = time.time()\n",
    "        print(\"Epoch time:\", round(t2 - t1), \"seconds\")\n",
    "        print(\"Loss:\", loss / i)\n",
    "        print(\"Evaluating model...\")\n",
    "        # new\n",
    "        # HR, NDCG = evaluate_model(model, data, validation=False)\n",
    "        # updated\n",
    "        # hr, ndcg = evaluate_model(model, data.test, top_K, random_samples)\n",
    "        # original\n",
    "        ht, ndcg = evaluate_model(\n",
    "            NCF,\n",
    "            data.test.values,\n",
    "            25,\n",
    "            random_samples,\n",
    "            data.num_movies\n",
    "        )\n",
    "        print(\"HR@i:\", hr)\n",
    "        print(\"NDCG@i\", ndcg)\n",
    "        loss = 0\n",
    "        print()\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import utils\n",
    "\n",
    "def fed_fit(model_central, data, C, batch_size, epochs, lr, eta, verbose=True):\n",
    "\n",
    "    # Sample the participants for the round of training\n",
    "    num_participants = int(data.num_users * C)\n",
    "    participants = random.sample(range(data.num_users), num_participants)\n",
    "\n",
    "    # model_difference holds the total change of the global model after the round\n",
    "    model_difference = copy.deepcopy(model_central)\n",
    "    utils.zero_model_parameters(model_difference)\n",
    "\n",
    "    it = 0\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Start training loop\n",
    "    for user in participants:\n",
    "\n",
    "        it += 1\n",
    "        if it % int(num_participants / 10) == 0 and verbose:\n",
    "            print(\"Progress:\", round(100 * it / num_participants), \"%\")\n",
    "\n",
    "        # The current user takes a copy of the global model\n",
    "        model_client = copy.deepcopy(model_central)\n",
    "\n",
    "        # Defining optimizers\n",
    "        optimizer = torch.optim.SGD(model_client.parameters(), lr=lr)  # MLP optimizer\n",
    "        optimizer_u = torch.optim.SGD(model_client.user_embedding.parameters(), lr=lr / C * eta - lr)  # User optimizer\n",
    "        optimizer_i = torch.optim.SGD(model_client.item_embedding.parameters(),\n",
    "                                      lr=lr * data.num_items * eta - lr)  # Item optimizer\n",
    "\n",
    "        # Prepares data for the current user\n",
    "        # data.set_current_user(user)\n",
    "        # data.generate_negatives()\n",
    "\n",
    "        dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                                shuffle=True, num_workers=0)\n",
    "\n",
    "        # Trains on the users data\n",
    "        for e in range(epochs):\n",
    "            for batch in dataloader:\n",
    "                # Load tensors of users, movies, outputs and loss weights\n",
    "                u, m, y = batch\n",
    "                # move tensors to cuda\n",
    "                u = u.to(device)\n",
    "                m = m.to(device)\n",
    "                y = y.to(device)\n",
    "                # w = w.to(device)\n",
    "\n",
    "                # make predictions\n",
    "                p_pred = model_client(u, m)\n",
    "\n",
    "                # Calculate mean loss\n",
    "                loss_fn = torch.nn.BCELoss()  # weight=w, reduction=\"mean\")\n",
    "                loss = loss_fn(p_pred, y)\n",
    "\n",
    "                # Backpropagate the output and update model parameters\n",
    "                optimizer.zero_grad()\n",
    "                optimizer_u.zero_grad()\n",
    "                optimizer_i.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer_u.step()\n",
    "                optimizer_i.step()\n",
    "\n",
    "        # Calculate the user's change of the model and add it to the total change\n",
    "        utils.sub_model_parameters(model_central, model_client)\n",
    "        utils.add_model_parameters(model_client, model_difference)\n",
    "\n",
    "    # Take the average of the MLP and item vectors\n",
    "    utils.divide_model_parameters(model_difference, num_participants)\n",
    "\n",
    "    # Update the global model by adding the total change\n",
    "    utils.add_model_parameters(model_difference, model_central)\n",
    "    t2 = time.time()\n",
    "    print(\"Time of round:\", round(t2 - t1), \"seconds\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# --- Train FedFit model  ---\n",
    "\n",
    "# Hyper parameters\n",
    "C = 0.4  # pop sample ratio\n",
    "eta = 80  # federated param\n",
    "E = 1  # epochs\n",
    "# B = 102  # batch size\n",
    "T = 196  # num rounds\n",
    "# lr = 0.3  # learning rate\n",
    "\n",
    "# Initiate model\n",
    "model_central = NCF(num_users=data.num_users, num_items=data.num_movies, embed_size=hidden_layers, output_size=output_size)\n",
    "\n",
    "print(\"Processing data...\")\n",
    "data = DataGenerator()\n",
    "print(\"Done\")\n",
    "\n",
    "for t in range(T):  # for each round\n",
    "    print(\"Starting round\", t + 1)\n",
    "    # train one round\n",
    "    fed_fit(model_central, data, C=C, batch_size=batch_size, epochs=E, lr=learning_rate, eta=eta, verbose=True)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    HR, NDCG = evaluate_model(model_central, data, validation=False)\n",
    "    print(\"HR@10:\", HR)\n",
    "    print(\"NDCG@10\", NDCG)\n",
    "\n",
    "torch.save(model_central.state_dict(), \"models/federatedNCF\")\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- Functions for training ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ncf = NCF(data.num_users, data.num_movies, emb_size, hidden_layers, output_size).to(device)\n",
    "\n",
    "train_model(ncf)\n",
    "\n",
    "torch.save(ncf.state_dict(), \"models/preTrained_NCF\")\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- TRAIN MODEL ( MINI BATCH ) ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' GET USER EMBEDDING '''\n",
    "from data import FairnessData\n",
    "\n",
    "sensitive = FairnessData()\n",
    "clean = sensitive.df\n",
    "\n",
    "train_ratio = 0.7\n",
    "\n",
    "train, test = sensitive.train_test_split(train_ratio)\n",
    "\n",
    "\n",
    "clean.info()  # 1476+3444 = 4920"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- GET USER EMBEDDING ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_embeds = ncf.user_emb.weight.data.cpu().detach().numpy()\n",
    "user_embeds = user_embeds.astype('float')\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' COMPUTE GENDER EMBEDDING '''\n",
    "gender_embed = np.zeros((2,user_embeds.shape[1]))\n",
    "num_users_x_group = np.zeros((2, 1))\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    u = train['uid'].iloc[i]\n",
    "    if train['gender'].iloc[i] == 0:\n",
    "        gender_embed[0] +=  user_embeds[u]\n",
    "        num_users_x_group[0] += 1.0\n",
    "    else:\n",
    "        gender_embed[1] +=  user_embeds[u]\n",
    "        gender_embed[1] += 1.0\n",
    "        num_users_x_group[1] += 1.0\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- COMPUTE GENDER EMBEDDING ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' VERTICAL BIAS'''\n",
    "gender_embed = gender_embed / num_users_x_group\n",
    "# vBias = compute_bias_direction(gender_embed)\n",
    "vBias = gender_embed[1].reshape((1,-1)) - gender_embed[0].reshape((1,-1))\n",
    "vBias = vBias / np.linalg.norm(vBias,axis=1,keepdims=1)\n",
    "\n",
    "vBias\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- VERTICAL BIAS ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "''' LINEAR PROJECTION '''\n",
    "\n",
    "# debias users\n",
    "debiased_user_embeds = user_embeds\n",
    "for i in range(len(clean)):\n",
    "    u = clean['uid'].iloc[i]\n",
    "    debiased_user_embeds[u] = user_embeds[u] - (np.inner(user_embeds[u].reshape(1,-1),vBias)[0][0])*vBias\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- LINEAAR PROJECTION ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''UPDATE USER EMBEDDINGS'''\n",
    "\n",
    "fairness_thres = torch.tensor(0.1).to(device)\n",
    "epsilonBase = torch.tensor(0.0).to(device)\n",
    "\n",
    "n_careers = sensitive.num_jobs\n",
    "\n",
    "# replace page items with career items\n",
    "ncf.like_emb = nn.Embedding(n_careers,emb_size).to(device)\n",
    "# freeze user embedding\n",
    "ncf.user_emb.weight.requires_grad=False\n",
    "\n",
    "# replace user embedding of the model with debiased embeddings\n",
    "ncf.user_emb.weight.data = torch.from_numpy(debiased_user_embeds.astype(np.float32)).to(device)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- Fine Tune for career optimization ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''OPTIMIZE'''\n",
    "# fair_fine_tune_model(DF_NCF,train_data, num_epochs, learning_rate,batch_size,num_negatives,n_careers,train_gender,fairness_thres,epsilonBase, unsqueeze=True)\n",
    "emb_size = 128\n",
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "num_negatives = 5\n",
    "\n",
    "random_samples = 15\n",
    "top_k = 10\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(ncf.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "ncf.train()\n",
    "\n",
    "torch.save(ncf.state_dict(), \"models/DF_NCF\")\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- OPTIMIZE ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''FAIR FINE TUNING MODEL'''\n",
    "all_users = torch.LongTensor(train['uid'].values).to(device)\n",
    "all_items = torch.LongTensor(train['job'].values).to(device)\n",
    "# protected attribute\n",
    "all_genders = torch.LongTensor(train['gender'].values).to(device)\n",
    "\n",
    "from fairness_measures import Measures\n",
    "m = Measures()\n",
    "\n",
    "num_batches = np.int64(np.floor(train.shape[0] / batch_size))\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    dataloader = DataLoader(sensitive, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=0)\n",
    "    t1 = time.time()\n",
    "\n",
    "    it_per_epoch = len(data) / batch_size\n",
    "    j = 1\n",
    "    for batch in dataloader:\n",
    "        u, j, g, r = batch\n",
    "        # move tensors to cuda\n",
    "        users = u.to(device)\n",
    "        jobs = m.to(device)\n",
    "        genders = g.to(device)\n",
    "        ratings = r.to(device)\n",
    "        \n",
    "        # users = torch.LongTensor(batch_df.uid.to_numpy()).to(device)\n",
    "        # items = torch.LongTensor(batch_df.job.to_numpy()).to(device)\n",
    "        # ratings = torch.FloatTensor(batch_df.rating.to_numpy()).to(device)\n",
    "        # print(items)\n",
    "        y_hat = ncf(users, jobs)\n",
    "\n",
    "        loss1 = nn.BCELoss(y_hat, ratings.unsqueeze(1))\n",
    "\n",
    "        predicted_probs = ncf(all_users, all_items)\n",
    "        avg_epsilon = m.computeEDF(all_genders,predicted_probs,n_careers,all_items,device)\n",
    "        print(avg_epsilon)\n",
    "        #criteroin hinge\n",
    "        loss2 = torch.max(torch.tensor(0.0).to(device), (avg_epsilon-epsilonBase))\n",
    "\n",
    "        loss = loss1 + fairness_thres*loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'epoch: {i + 1} \\nbatch: {j} out of: {num_batches} \\naverage loss: {loss.item()}\\n')\n",
    "        j+=1\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- FINE TUNING ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(ncf.state_dict(), \"models/DF_NCF\")\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- SAVE ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "def evaluate_fine_tune(model,df_val,top_K,random_samples):\n",
    "    model.eval()\n",
    "    avg_HR = np.zeros((len(df_val),top_K))\n",
    "    avg_NDCG = np.zeros((len(df_val),top_K))\n",
    "\n",
    "    # for i in range(len(df_val)):\n",
    "    test_df = sensitive.add_negatives(\n",
    "        df_val,\n",
    "        item='job',\n",
    "        items=sensitive.jobs,\n",
    "        n_samples=random_samples\n",
    "    )\n",
    "    users, items = torch.LongTensor(test_df.uid).to(device), torch.LongTensor(test_df.job).to(device)\n",
    "    y_hat = model(users, items)\n",
    "\n",
    "    y_hat = y_hat.cpu().detach().numpy().reshape((-1,))\n",
    "    items = items.cpu().detach().numpy().reshape((-1,))\n",
    "    map_item_score = {}\n",
    "    for j in range(len(y_hat)):\n",
    "        map_item_score[items[j]] = y_hat[j]\n",
    "    for k in range(top_K):\n",
    "        # Evaluate top rank list\n",
    "        ranklist = heapq.nlargest(k, map_item_score, key=map_item_score.get)\n",
    "        gtItem = items[0]\n",
    "        avg_HR[i,k] = getHitRatio(ranklist, gtItem)\n",
    "        avg_NDCG[i,k] = getNDCG(ranklist, gtItem)\n",
    "    avg_HR = np.mean(avg_HR, axis = 0)\n",
    "    avg_NDCG = np.mean(avg_NDCG, axis = 0)\n",
    "    return avg_HR, avg_NDCG\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- Functions for evaluating model ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''EVALUATE TUNED MODEL'''\n",
    "hr, ndcg = evaluate_fine_tune(ncf, test, top_K, random_samples)\n",
    "\n",
    "hr\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- Evaluate Model ---\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''MEASURE THE FAIRNESS OF THE MODEL'''\n",
    "def fairness_measures(model,df_val,num_items):\n",
    "    model.eval()\n",
    "    users, items = torch.LongTensor(df_val.uid.to_numpy()).to(device), torch.LongTensor(df_val.job.to_numpy()).to(device)\n",
    "    y_hat = model(users, items)\n",
    "\n",
    "    avg_epsilon = m.computeEDF(all_genders.cpu(),y_hat,num_items,items,device)\n",
    "    U_abs = m.compute_absolute_unfairness(all_genders.cpu(),y_hat,num_items,items,device)\n",
    "\n",
    "    avg_epsilon = avg_epsilon.cpu().detach().numpy().reshape((-1,)).item()\n",
    "    print(f\"average differential fairness: {avg_epsilon: .3f}\")\n",
    "\n",
    "    U_abs = U_abs.cpu().detach().numpy().reshape((-1,)).item()\n",
    "    print(f\"absolute unfairness: {U_abs: .3f}\")\n",
    "\n",
    "fairness_measures(ncf, test, n_careers)\n",
    "\n",
    "# -----------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% --- Function for evaluating fairness ---\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}