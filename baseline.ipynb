{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import heapq  # for retrieval topK\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from numpy.random import choice"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class neuralCollabFilter(nn.Module):\n",
    "    def __init__(self, num_users, num_likes, embed_size, num_hidden, output_size):\n",
    "        super(neuralCollabFilter, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embed_size)\n",
    "        self.like_emb = nn.Embedding(num_likes, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size * 2, num_hidden[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(num_hidden[0], num_hidden[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(num_hidden[1], num_hidden[2])\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(num_hidden[2], num_hidden[3])\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.outLayer = nn.Linear(num_hidden[3], output_size)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        U = self.user_emb(u)\n",
    "        V = self.like_emb(v)\n",
    "        out = torch.cat([U, V], dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.outLayer(out)\n",
    "        out = self.out_act(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_instances_with_random_neg_samples(train, num_items, num_negatives, device):\n",
    "    user_input = np.zeros((len(train) + len(train) * num_negatives))\n",
    "    item_input = np.zeros((len(train) + len(train) * num_negatives))\n",
    "    labels = np.zeros((len(train) + len(train) * num_negatives))\n",
    "\n",
    "    neg_samples = choice(num_items, size=(\n",
    "        10 * len(train) * num_negatives,))  # multiply by 2 to make sure, we dont run out of negative samples\n",
    "    neg_counter = 0\n",
    "    i = 0\n",
    "    for n in range(len(train)):\n",
    "        # positive instance\n",
    "        user_input[i] = train['user_id'][n]\n",
    "        item_input[i] = train['like_id'][n]\n",
    "        labels[i] = 1\n",
    "        i += 1\n",
    "        # negative instances\n",
    "        checkList = list(train['like_id'][train['user_id'] == train['user_id'][n]])\n",
    "        for t in range(num_negatives):\n",
    "            j = neg_samples[neg_counter]\n",
    "            while j in checkList:\n",
    "                neg_counter += 1\n",
    "                j = neg_samples[neg_counter]\n",
    "            user_input[i] = train['user_id'][n]\n",
    "            item_input[i] = j\n",
    "            labels[i] = 0\n",
    "            i += 1\n",
    "            neg_counter += 1\n",
    "    return torch.LongTensor(user_input).to(device), torch.LongTensor(item_input).to(device), torch.FloatTensor(\n",
    "        labels).to(device)\n",
    "\n",
    "\n",
    "def get_test_instances_with_random_samples(data, random_samples, num_items, device):\n",
    "    user_input = np.zeros((random_samples + 1))\n",
    "    item_input = np.zeros((random_samples + 1))\n",
    "\n",
    "    # positive instance\n",
    "    user_input[0] = data[0]\n",
    "    item_input[0] = data[1]\n",
    "    i = 1\n",
    "    # negative instances\n",
    "    checkList = data[1]\n",
    "    for t in range(random_samples):\n",
    "        j = np.random.randint(num_items)\n",
    "        while j == checkList:\n",
    "            j = np.random.randint(num_items)\n",
    "        user_input[i] = data[0]\n",
    "        item_input[i] = j\n",
    "        i += 1\n",
    "    return torch.LongTensor(user_input).to(device), torch.LongTensor(item_input).to(device)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i + 2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "# The function below ensures that we seed all random generators with the same value to get reproducible results\n",
    "def set_random_seed(state=1):\n",
    "    gens = (np.random.seed, torch.manual_seed, torch.cuda.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "set_random_seed(RANDOM_STATE)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epochs(model, df_train, epochs, lr, batch_size, num_negatives, unsqueeze=False):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        permutation = torch.randperm(df_train.shape[0])\n",
    "        for mini_batch in range(0, df_train.shape[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            print(mini_batch)\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            print(indices)\n",
    "            batch = df_train[indices].reset_index(drop=True)\n",
    "            train_user_input, train_item_input, train_ratings = get_instances_with_random_neg_samples(\n",
    "                batch,\n",
    "                num_uniqueLikes,\n",
    "                num_negatives,\n",
    "                device\n",
    "            )\n",
    "            train_ratings = train_ratings.unsqueeze(1)\n",
    "            y_hat = model(train_user_input, train_item_input)\n",
    "            loss = criterion(y_hat, train_ratings)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i%5==0:\n",
    "                print('epoch: ', i, 'batch: ', mini_batch, 'out of: ', np.int64(np.floor(len(df_train) / batch_size)),\n",
    "                      'average loss: ', loss.item())\n",
    "\n",
    "\n",
    "# %% model evaluation: hit rate and NDCG\n",
    "def evaluate_model(model, df_val, top_K, random_samples, num_items):\n",
    "    model.eval()\n",
    "    avg_HR = np.zeros((len(df_val), top_K))\n",
    "    avg_NDCG = np.zeros((len(df_val), top_K))\n",
    "\n",
    "    for i in range(len(df_val)):\n",
    "        test_user_input, test_item_input = get_test_instances_with_random_samples(df_val[i], random_samples, num_items,\n",
    "                                                                                  device)\n",
    "        y_hat = model(test_user_input, test_item_input)\n",
    "        y_hat = y_hat.cpu().detach().numpy().reshape((-1,))\n",
    "        test_item_input = test_item_input.cpu().detach().numpy().reshape((-1,))\n",
    "        map_item_score = {}\n",
    "        for j in range(len(y_hat)):\n",
    "            map_item_score[test_item_input[j]] = y_hat[j]\n",
    "        for k in range(top_K):\n",
    "            # Evaluate top rank list\n",
    "            ranklist = heapq.nlargest(k, map_item_score, key=map_item_score.get)\n",
    "            gtItem = test_item_input[0]\n",
    "            avg_HR[i, k] = getHitRatio(ranklist, gtItem)\n",
    "            avg_NDCG[i, k] = getNDCG(ranklist, gtItem)\n",
    "    avg_HR = np.mean(avg_HR, axis=0)\n",
    "    avg_NDCG = np.mean(avg_NDCG, axis=0)\n",
    "    return avg_HR, avg_NDCG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% pre-training NCF model with user-page pairs\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %% load data\n",
    "train_data = pd.read_csv(\"train-test/train_userPages.csv\")\n",
    "test_data = pd.read_csv(\"train-test/test_userPages.csv\")\n",
    "\n",
    "# %% set hyperparameters\n",
    "emb_size = 128\n",
    "hidden_layers = np.array([emb_size, 64, 32, 16])\n",
    "output_size = 1\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "batch_size = 2048\n",
    "num_negatives = 5\n",
    "\n",
    "random_samples = 100\n",
    "top_K = 10\n",
    "\n",
    "num_uniqueUsers = len(train_data.user_id.unique())\n",
    "num_uniqueLikes = len(train_data.like_id.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preTrained_NCF = neuralCollabFilter(num_uniqueUsers, num_uniqueLikes, emb_size, hidden_layers, output_size).to(device)\n",
    "preTrained_NCF.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(preTrained_NCF.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "preTrained_NCF.train()\n",
    "data_batch = (train_data[0:(0 + batch_size)]).reset_index(drop=True)\n",
    "# train_user_input, train_item_input, train_ratings = get_instances_with_neg_samples(data_batch, probabilities, num_negatives,device)\n",
    "train_user_input, train_item_input, train_ratings = get_instances_with_random_neg_samples(\n",
    "    data_batch,\n",
    "    num_uniqueLikes,\n",
    "    num_negatives,\n",
    "    device\n",
    ")\n",
    "train_ratings = train_ratings.unsqueeze(1)\n",
    "y_hat = preTrained_NCF(train_user_input, train_item_input)\n",
    "loss = criterion(y_hat, train_ratings)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[train_data.user_id==4].like_id.sort_values(ascending=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preTrained_NCF = neuralCollabFilter(num_uniqueUsers, num_uniqueLikes, emb_size, hidden_layers, output_size).to(device)\n",
    "preTrained_NCF.cuda()\n",
    "train_epochs(preTrained_NCF, train_data, num_epochs, learning_rate, batch_size, num_negatives, unsqueeze=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% start training the NCF model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(preTrained_NCF.state_dict(), \"trained-models/preTrained_NCF\")\n",
    "\n",
    "# %% evaluate the model\n",
    "\n",
    "avg_HR_preTrain, avg_NDCG_preTrain = evaluate_model(preTrained_NCF, test_data.values, top_K, random_samples,\n",
    "                                                    num_uniqueLikes)\n",
    "\n",
    "# np.savetxt('results/avg_HR_preTrain.txt', avg_HR_preTrain)\n",
    "# np.savetxt('results/avg_NDCG_preTrain.txt', avg_NDCG_preTrain)\n",
    "\n",
    "# sys.stdout.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_gender_direction(data, S, user_vectors):\n",
    "    genderEmbed = np.zeros((2,user_vectors.shape[1]))\n",
    "    # S = 0 indicates male and S = 1 indicates female\n",
    "    num_users_per_group = np.zeros((2,1))\n",
    "    for i in range(len(data)):\n",
    "        u = data['user_id'][i]\n",
    "        if S['gender'][i] == 0:\n",
    "            genderEmbed[0] +=  user_vectors[u]\n",
    "            num_users_per_group[0] += 1.0\n",
    "        else:\n",
    "            genderEmbed[1] +=  user_vectors[u]\n",
    "            num_users_per_group[1] += 1.0\n",
    "\n",
    "    genderEmbed = genderEmbed / num_users_per_group # average gender embedding\n",
    "    return genderEmbed\n",
    "\n",
    "def compute_bias_direction(gender_vectors):\n",
    "    vBias= gender_vectors[1].reshape((1,-1))-gender_vectors[0].reshape((1,-1))\n",
    "    vBias = vBias / np.linalg.norm(vBias,axis=1,keepdims=1)\n",
    "    return vBias\n",
    "\n",
    "def linear_projection(data,user_vectors,vBias):\n",
    "    # linear projection: u - <u,v_b>v_b\n",
    "    for i in range(len(data)):\n",
    "        u = data['user_id'][i]\n",
    "        user_vectors[u] = user_vectors[u] - (np.inner(user_vectors[u].reshape(1,-1),vBias)[0][0])*vBias\n",
    "    return user_vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% compute bias direction & linear projection\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_users= pd.read_csv(\"train-test/train_usersID.csv\",names=['user_id'])\n",
    "test_users = pd.read_csv(\"train-test/test_usersID.csv\",names=['user_id'])\n",
    "\n",
    "train_careers= pd.read_csv(\"train-test/train_concentrationsID.csv\",names=['like_id'])\n",
    "test_careers = pd.read_csv(\"train-test/test_concentrationsID.csv\",names=['like_id'])\n",
    "\n",
    "train_protected_attributes= pd.read_csv(\"train-test/train_protectedAttributes.csv\")\n",
    "test_protected_attributes = pd.read_csv(\"train-test/test_protectedAttributes.csv\")\n",
    "\n",
    "unique_careers= pd.read_csv(\"train-test/unique_careers.csv\")\n",
    "train_userPages = pd.read_csv(\"train-test/train_userPages.csv\")\n",
    "\n",
    "train_data = (pd.concat([train_users['user_id'],train_careers['like_id']],axis=1)).reset_index(drop=True)\n",
    "test_data = (pd.concat([test_users['user_id'],test_careers['like_id']],axis=1)).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emb_size = 128\n",
    "hidden_layers = np.array([emb_size, 64, 32, 16])\n",
    "output_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "num_negatives = 5\n",
    "\n",
    "random_samples = 100\n",
    "top_K = 25\n",
    "\n",
    "# to load pre-train model correctly\n",
    "num_uniqueUsers = len(train_userPages.user_id.unique())\n",
    "num_uniqueLikes = len(train_userPages.like_id.unique())\n",
    "\n",
    "# to fine tune career recommendation\n",
    "num_uniqueCareers = len(train_data.like_id.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% set hyperparameters\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "debiased_NCF = neuralCollabFilter(num_uniqueUsers, num_uniqueLikes, emb_size, hidden_layers,output_size).to(device)\n",
    "debiased_NCF.load_state_dict(torch.load(\"trained-models/preTrained_NCF\"))\n",
    "debiased_NCF.to(device)\n",
    "users_embed = debiased_NCF.user_emb.weight.data.cpu().detach().numpy()\n",
    "users_embed = users_embed.astype('float')\n",
    "np.savetxt('results/users_embed.txt',users_embed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load pre-trained model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gender_embed = compute_gender_direction(train_data, train_protected_attributes, users_embed)\n",
    "# np.savetxt('results/gender_embed.txt',gender_embed)\n",
    "\n",
    "vBias = compute_bias_direction(gender_embed)\n",
    "# np.savetxt('results/vBias.txt',vBias)\n",
    "\n",
    "# incorporate all users: debias train & test both\n",
    "all_data = (pd.concat([train_data,test_data],axis=0)).reset_index(drop=True)\n",
    "debias_users_embed = linear_projection(all_data,users_embed,vBias) # first debias training users\n",
    "#debias_users_embed = linear_projection(test_data,debias_users_embed,vBias) # then debias test users\n",
    "# np.savetxt('results/debias_users_embed.txt',debias_users_embed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% compute bias direction on training users and debias user embeds using linear projection\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''CAREER RECOMMEND'''\n",
    "def criterionHinge(epsilonClass, epsilonBase):\n",
    "    zeroTerm = torch.tensor(0.0).to(device)\n",
    "    return torch.max(zeroTerm, (epsilonClass-epsilonBase))\n",
    "\n",
    "def computeEDF(protectedAttributes,predictions,numClasses,item_input,device):\n",
    "    # compute counts and probabilities\n",
    "    S = np.unique(protectedAttributes) # number of gender: male = 0; female = 1\n",
    "    countsClassOne = torch.zeros((numClasses,len(S)),dtype=torch.float).to(device) #each entry corresponds to an intersection, arrays sized by largest number of values\n",
    "    countsTotal = torch.zeros((numClasses,len(S)),dtype=torch.float).to(device)\n",
    "\n",
    "    concentrationParameter = 1.0\n",
    "    dirichletAlpha = concentrationParameter/numClasses\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        countsTotal[item_input[i],protectedAttributes[i]] = countsTotal[item_input[i],protectedAttributes[i]] + 1.0\n",
    "        countsClassOne[item_input[i],protectedAttributes[i]] = countsClassOne[item_input[i],protectedAttributes[i]] + predictions[i]\n",
    "\n",
    "    #probabilitiesClassOne = countsClassOne/countsTotal\n",
    "    probabilitiesForDFSmoothed = (countsClassOne + dirichletAlpha) /(countsTotal + concentrationParameter)\n",
    "    avg_epsilon = differentialFairnessMultiClass(probabilitiesForDFSmoothed,numClasses,device)\n",
    "    return avg_epsilon\n",
    "def differentialFairnessMultiClass(probabilitiesOfPositive,numClasses,device):\n",
    "    # input: probabilitiesOfPositive = positive p(y|S) from ML algorithm\n",
    "    # output: epsilon = differential fairness measure\n",
    "    epsilonPerClass = torch.zeros(len(probabilitiesOfPositive),dtype=torch.float).to(device)\n",
    "    for c in  range(len(probabilitiesOfPositive)):\n",
    "        epsilon = torch.tensor(0.0).to(device) # initialization of DF\n",
    "        for i in  range(len(probabilitiesOfPositive[c])):\n",
    "            for j in range(len(probabilitiesOfPositive[c])):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                else:\n",
    "                    epsilon = torch.max(epsilon,torch.abs(torch.log(probabilitiesOfPositive[c,i])-torch.log(probabilitiesOfPositive[c,j]))) # ratio of probabilities of positive outcome\n",
    "        #                    epsilon = torch.max(epsilon,torch.abs((torch.log(1-probabilitiesOfPositive[c,i]))-(torch.log(1-probabilitiesOfPositive[c,j])))) # ratio of probabilities of negative outcome\n",
    "        epsilonPerClass[c] = epsilon # overall DF of the algorithm\n",
    "    avg_epsilon = torch.mean(epsilonPerClass)\n",
    "    return avg_epsilon\n",
    "\n",
    "def computeAbsoluteUnfairness(protectedAttributes,predictions,numClasses,item_input,device):\n",
    "    # compute counts and probabilities\n",
    "    S = np.unique(protectedAttributes) # number of gender: male = 0; female = 1\n",
    "    scorePerGroupPerItem = torch.zeros((numClasses,len(S)),dtype=torch.float).to(device) #each entry corresponds to an intersection, arrays sized by largest number of values\n",
    "    scorePerGroup = torch.zeros(len(S),dtype=torch.float).to(device)\n",
    "    countPerItem = torch.zeros((numClasses,len(S)),dtype=torch.float).to(device)\n",
    "\n",
    "    concentrationParameter = 1.0\n",
    "    dirichletAlpha = concentrationParameter/numClasses\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        scorePerGroupPerItem[item_input[i],protectedAttributes[i]] = scorePerGroupPerItem[item_input[i],protectedAttributes[i]] + predictions[i]\n",
    "        countPerItem[item_input[i],protectedAttributes[i]] = countPerItem[item_input[i],protectedAttributes[i]] + 1.0\n",
    "        scorePerGroup[protectedAttributes[i]] = scorePerGroup[protectedAttributes[i]] + predictions[i]\n",
    "    #probabilitiesClassOne = countsClassOne/countsTotal\n",
    "    avgScorePerGroupPerItem = (scorePerGroupPerItem + dirichletAlpha) /(countPerItem + concentrationParameter)\n",
    "    avg_score = scorePerGroup/torch.sum(countPerItem,axis=0)  #torch.mean(avgScorePerGroupPerItem,axis=0)\n",
    "    difference = torch.abs(avgScorePerGroupPerItem - avg_score)\n",
    "    U_abs = torch.mean(torch.abs(difference[:,0]-difference[:,1]))\n",
    "    return U_abs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fair_fine_tune_model(model,df_train, epochs, lr,batch_size,num_negatives,num_items,protectedAttributes,lamda,epsilonBase,unsqueeze=False):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    model.train()\n",
    "\n",
    "    all_user_input = torch.LongTensor(df_train['user_id'].values).to(device)\n",
    "    all_item_input = torch.LongTensor(df_train['like_id'].values).to(device)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        j = 0\n",
    "        for batch_i in range(0,np.int64(np.floor(len(df_train)/batch_size))*batch_size,batch_size):\n",
    "            data_batch = (df_train[batch_i:(batch_i+batch_size)]).reset_index(drop=True)\n",
    "            train_user_input, train_item_input, train_ratings = get_instances_with_random_neg_samples(data_batch, num_items, num_negatives,device)\n",
    "            if unsqueeze:\n",
    "                train_ratings = train_ratings.unsqueeze(1)\n",
    "            y_hat = model(train_user_input, train_item_input)\n",
    "            loss1 = criterion(y_hat, train_ratings)\n",
    "\n",
    "            predicted_probs = model(all_user_input, all_item_input)\n",
    "            avg_epsilon = computeEDF(protectedAttributes,predicted_probs,num_items,all_item_input,device)\n",
    "            loss2 = criterionHinge(avg_epsilon, epsilonBase)\n",
    "\n",
    "            loss = loss1 + lamda*loss2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('epoch: ', i, 'batch: ', j, 'out of: ',np.int64(np.floor(len(df_train)/batch_size)), 'average loss: ',loss.item())\n",
    "            j = j+1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% fine-tuning pre-trained model with user-career pairs\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_fine_tune(model,df_val,top_K,random_samples, num_items):\n",
    "    model.eval()\n",
    "    avg_HR = np.zeros((len(df_val),top_K))\n",
    "    avg_NDCG = np.zeros((len(df_val),top_K))\n",
    "\n",
    "    for i in range(len(df_val)):\n",
    "        test_user_input, test_item_input = get_test_instances_with_random_samples(df_val[i], random_samples,num_items,device)\n",
    "        y_hat = model(test_user_input, test_item_input)\n",
    "        y_hat = y_hat.cpu().detach().numpy().reshape((-1,))\n",
    "        test_item_input = test_item_input.cpu().detach().numpy().reshape((-1,))\n",
    "        map_item_score = {}\n",
    "        for j in range(len(y_hat)):\n",
    "            map_item_score[test_item_input[j]] = y_hat[j]\n",
    "        for k in range(top_K):\n",
    "            # Evaluate top rank list\n",
    "            ranklist = heapq.nlargest(k, map_item_score, key=map_item_score.get)\n",
    "            gtItem = test_item_input[0]\n",
    "            avg_HR[i,k] = getHitRatio(ranklist, gtItem)\n",
    "            avg_NDCG[i,k] = getNDCG(ranklist, gtItem)\n",
    "    avg_HR = np.mean(avg_HR, axis = 0)\n",
    "    avg_NDCG = np.mean(avg_NDCG, axis = 0)\n",
    "    return avg_HR, avg_NDCG"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% model evaluation: hit rate and NDCG\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fairness_measures(model,df_val,num_items,protectedAttributes):\n",
    "    model.eval()\n",
    "    user_input = torch.LongTensor(df_val['user_id'].values).to(device)\n",
    "    item_input = torch.LongTensor(df_val['like_id'].values).to(device)\n",
    "    y_hat = model(user_input, item_input)\n",
    "\n",
    "    avg_epsilon = computeEDF(protectedAttributes,y_hat,num_items,item_input,device)\n",
    "    U_abs = computeAbsoluteUnfairness(protectedAttributes,y_hat,num_items,item_input,device)\n",
    "\n",
    "    avg_epsilon = avg_epsilon.cpu().detach().numpy().reshape((-1,)).item()\n",
    "    print(f\"average differential fairness: {avg_epsilon: .3f}\")\n",
    "\n",
    "    U_abs = U_abs.cpu().detach().numpy().reshape((-1,)).item()\n",
    "    print(f\"absolute unfairness: {U_abs: .3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_users= pd.read_csv(\"train-test/train_usersID.csv\",names=['user_id'])\n",
    "test_users = pd.read_csv(\"train-test/test_usersID.csv\",names=['user_id'])\n",
    "\n",
    "train_careers= pd.read_csv(\"train-test/train_concentrationsID.csv\",names=['like_id'])\n",
    "test_careers = pd.read_csv(\"train-test/test_concentrationsID.csv\",names=['like_id'])\n",
    "\n",
    "train_protected_attributes= pd.read_csv(\"train-test/train_protectedAttributes.csv\")\n",
    "test_protected_attributes = pd.read_csv(\"train-test/test_protectedAttributes.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# train_labels= pd.read_csv(\"train-test/train_labels.csv\",names=['labels'])\n",
    "# test_labels = pd.read_csv(\"train-test/test_labels.csv\",names=['labels'])\n",
    "#\n",
    "# unique_concentrations = (pd.concat([train_careers['like_id'],train_labels['labels']],axis=1)).reset_index(drop=True)\n",
    "# unique_concentrations = unique_concentrations.drop_duplicates(subset='like_id', keep='first')\n",
    "#\n",
    "# unique_careers = unique_concentrations.sort_values(by=['like_id']).reset_index(drop=True)\n",
    "# unique_careers.to_csv('train-test/unique_careers.csv',index=False)\n",
    "# =============================================================================\n",
    "unique_careers= pd.read_csv(\"train-test/unique_careers.csv\")\n",
    "train_userPages = pd.read_csv(\"train-test/train_userPages.csv\")\n",
    "\n",
    "train_data = (pd.concat([train_users['user_id'],train_careers['like_id']],axis=1)).reset_index(drop=True)\n",
    "test_data = (pd.concat([test_users['user_id'],test_careers['like_id']],axis=1)).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "emb_size = 128\n",
    "hidden_layers = np.array([emb_size, 64, 32, 16])\n",
    "output_size = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "num_negatives = 5\n",
    "\n",
    "random_samples = 15\n",
    "top_K = 10\n",
    "\n",
    "# to load pre-train model correctly\n",
    "num_uniqueUsers = len(train_userPages.user_id.unique())\n",
    "num_uniqueLikes = len(train_userPages.like_id.unique())\n",
    "\n",
    "# to fine tune career recommendation\n",
    "num_uniqueCareers = len(train_data.like_id.unique())\n",
    "\n",
    "train_gender = train_protected_attributes['gender'].values\n",
    "test_gender = test_protected_attributes['gender'].values\n",
    "\n",
    "fairness_thres = torch.tensor(0.1).to(device)\n",
    "epsilonBase = torch.tensor(0.0).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% set hyperparameters\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DF_NCF = neuralCollabFilter(num_uniqueUsers, num_uniqueLikes, emb_size, hidden_layers,output_size).to(device)\n",
    "\n",
    "DF_NCF.load_state_dict(torch.load(\"trained-models/preTrained_NCF\"))\n",
    "\n",
    "DF_NCF.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% load pre-trained model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# replace page items with career items\n",
    "DF_NCF.like_emb = nn.Embedding(num_uniqueCareers,emb_size).to(device)\n",
    "# freeze user embedding\n",
    "DF_NCF.user_emb.weight.requires_grad=False\n",
    "# load debiased user embeddings\n",
    "debias_users_embed = np.loadtxt('results/debias_users_embed.txt')\n",
    "# replace user embedding of the model with debiased embeddings\n",
    "DF_NCF.user_emb.weight.data = torch.from_numpy(debias_users_embed.astype(np.float32)).to(device)\n",
    "\n",
    "fair_fine_tune_model(DF_NCF,train_data, num_epochs, learning_rate,batch_size,num_negatives,num_uniqueCareers,train_gender,fairness_thres,epsilonBase, unsqueeze=True)\n",
    "\n",
    "torch.save(DF_NCF.state_dict(), \"trained-models/DF_NCF\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% fine-tune to career recommendation\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout=open(\"NFCF_output.txt\",\"w\")\n",
    "\n",
    "avg_HR_DF_NCF, avg_NDCG_DF_NCF = evaluate_fine_tune(DF_NCF,test_data.values,top_K,random_samples, num_uniqueCareers)\n",
    "\n",
    "np.savetxt('results/avg_HR_NFCF.txt',avg_HR_DF_NCF)\n",
    "np.savetxt('results/avg_NDCG_NFCF.txt',avg_NDCG_DF_NCF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% evaluate the fine-tune model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fairness_measures(DF_NCF,test_data,num_uniqueCareers,test_gender)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% fairness measurements\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}