{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from random import sample\n",
    "# import tensorflow as tf\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import T_co\n",
    "\n",
    "from torch import LongTensor\n",
    "\n",
    "\n",
    "class TargetData(Dataset):\n",
    "    def __init__(self, num_negatives=4):\n",
    "        self.num_jobs = 0\n",
    "        self.df = self.load_ratings()\n",
    "        self.num_users = self.df.uid.nunique()\n",
    "        self.num_movies = self.df.mid.nunique()\n",
    "        self.users = set(self.df.uid.unique())\n",
    "        self.movies = set(self.df.mid.unique())\n",
    "\n",
    "        self.train, self.test = self._train_test_split()\n",
    "\n",
    "        self.training_data = self.add_negatives(self.train, items=self.movies,  n_samples=num_negatives)\n",
    "        self.testing_data = self.add_negatives(self.test, items=self.movies, n_samples=100)\n",
    "\n",
    "        self.testing_tensors = self.parse_testing(self.testing_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.training_data.shape[0]  # Length of the data to train on\n",
    "\n",
    "    def __getitem__(self, index) -> T_co:\n",
    "        user = LongTensor([self.training_data.uid.iloc[index]])  # -1 so that indexing starts from 0\n",
    "        movie = LongTensor([self.training_data.mid.iloc[index]])\n",
    "        output = LongTensor([self.training_data.rating.iloc[index]])\n",
    "        return user, movie, output\n",
    "\n",
    "    def __call__(self, test_data):\n",
    "        return self.parse_testing(self.add_negatives(test_data, items=self.movies, n_samples=100))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_ratings(min_ratings=5):\n",
    "        df = pd.read_csv('MovieLens/ratings.dat',\n",
    "                         sep='::',\n",
    "                         header=None,\n",
    "                         names=['uid_old', 'mid_old', 'rating', 'date'],\n",
    "                         parse_dates=['date'],\n",
    "                         date_parser=lambda x: pd.to_datetime(x, unit='s', origin='unix'),\n",
    "                         engine='python')\n",
    "\n",
    "        # DROP MOVIES WITH LESS THAN 5 RATINGS\n",
    "        s = df.groupby(['mid_old']).size()\n",
    "        low_n_ratings = s[s < min_ratings].reset_index().mid_old.tolist()\n",
    "        df = df[~df.mid_old.isin(low_n_ratings)]\n",
    "        # RE-INDEX USERS AND MOVIES\n",
    "        user_id = df[['uid_old']].drop_duplicates().reindex()\n",
    "        user_id['uid'] = np.arange(len(user_id))\n",
    "        df = pd.merge(df, user_id, on=['uid_old'], how='left')\n",
    "\n",
    "        item_id = df[['mid_old']].drop_duplicates()\n",
    "        item_id['mid'] = np.arange(len(item_id))\n",
    "        return pd.merge(df, item_id, on=['mid_old'], how='left')\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_testing(df):\n",
    "        test = df.sort_values(by=['uid', 'rating'], ascending=False)\n",
    "        users, movies, outputs = [], [], []\n",
    "        for _, u in test.groupby('uid'):\n",
    "            users.append(LongTensor([u.uid.values]))\n",
    "            movies.append(LongTensor([u.mid.values]))\n",
    "            outputs.append(LongTensor([u.rating.values]))\n",
    "        return users, movies, outputs\n",
    "\n",
    "    def _train_test_split(self):\n",
    "        self.df.rating = np.int8(1)\n",
    "        self.df['latest'] = self.df.groupby(['uid'])['date'].rank(method='first', ascending=False)\n",
    "        test_bool = self.df.latest == 1\n",
    "        test = self.df[test_bool]\n",
    "        train = self.df[~test_bool]\n",
    "        return (train[['uid', 'mid', 'rating']],\n",
    "                test[['uid', 'mid', 'rating']]\n",
    "                )\n",
    "\n",
    "    def add_negatives(self, df: pd.DataFrame, item: str = 'mid', items=None, n_samples: int = 4):\n",
    "        if items is None:\n",
    "            items = set(self.train[item].unique())\n",
    "\n",
    "        combine = df.groupby('uid')[item].apply(set).reset_index()\n",
    "        combine['negatives'] = combine[item].apply(lambda x: sample(list(items - x), n_samples))\n",
    "\n",
    "        s = combine.apply(lambda x: pd.Series(x.negatives, dtype=np.int16), axis=1).stack().reset_index()\n",
    "        s.rename(columns={'level_0': 'uid', 0: item}, inplace=True)\n",
    "        s.drop(['level_1'], axis=1, inplace=True)\n",
    "        s['rating'] = np.int8(0)\n",
    "        s.uid = s.uid.astype(np.int16)\n",
    "\n",
    "        complete = pd.concat([df, s]).sort_values(by=['uid', item])\n",
    "        return complete.reset_index(drop=True)\n",
    "\n",
    "\n",
    "class AttributeData(Dataset):\n",
    "    def __init__(self, num_negatives: int = 4, training_ratio: float = .8):\n",
    "        self.targets = TargetData()\n",
    "        self.df = pd.merge(\n",
    "            self._features(),\n",
    "            self.targets.train,\n",
    "            # on=['uid', 'uid'],\n",
    "            how='left'\n",
    "            )[['uid', 'mid', 'age', 'gender', 'job', 'rating']]\n",
    "\n",
    "        # self.df.rename(columns={'rating_x': 'rating'}, inplace=True)\n",
    "\n",
    "        self.num_users = self.df.uid.nunique()\n",
    "        self.num_jobs = self.df.job.nunique()\n",
    "        self.jobs = set(self.df.job.unique())\n",
    "        self.train, self.test = self._train_test_split()\n",
    "\n",
    "        self.training_data = self.add_negatives(\n",
    "            self.train,\n",
    "            item='job',\n",
    "            items=self.jobs,\n",
    "            n_samples=num_negatives)\n",
    "        self.training_data['age'] = self.training_data.groupby('uid')['age'].transform('first')\n",
    "        self.training_data['gender'] = self.training_data.groupby('uid')['gender'].transform('first')\n",
    "        self.training_data.dropna(inplace=True)\n",
    "        # self.training_data['mid'] = self.training_data.apply(lambda x: np.random.choice(1) if x.rating == 0)\n",
    "        # self.training_data['job'] = self.training_data.groupby('uid')['job'].transform('first')\n",
    "        # self.jobs_train, self.genders_train, self.ages_train = self.perturb_input(self.training_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train.shape[0]  # Length of the data to train on\n",
    "\n",
    "    def __getitem__(self, index) -> T_co:\n",
    "        # features = LongTensor(self.training_data.iloc[index, 1:-1])\n",
    "        user = LongTensor(self.training_data.uid.iloc[index])\n",
    "        # job = LongTensor(self.training_data.iloc[index, :])\n",
    "        # gender = LongTensor(self.training_data.iloc[index, :])\n",
    "        # age = LongTensor(self.training_data.iloc[index])\n",
    "        rating = LongTensor(self.training_data.rating.iloc[index])\n",
    "        return user, rating\n",
    "\n",
    "    # def __getitem__(self, index) -> T_co:\n",
    "    #     user = LongTensor(self.training_data.uid.iloc[index])\n",
    "    #     job = LongTensor(self.training_data.job.iloc[index])\n",
    "    #     # protected attribute\n",
    "    #     gender = LongTensor(self.training_data.gender.iloc[index])\n",
    "    #     age = LongTensor(self.training_data.age.iloc[index])\n",
    "    #     rating = LongTensor(self.training_data.rating.iloc[index])\n",
    "    #     return user, job, gender, age, rating\n",
    "\n",
    "    # def __call__(self, test_data):\n",
    "    #     return self.parse_testing(self.add_negatives(test_data, items=self.movies, n_samples=100))\n",
    "\n",
    "    def perturb_input(self, df):\n",
    "        # HOT ENCODING (CATEGORICAL)\n",
    "        func1, func2 = self.obfuscation_functions()\n",
    "\n",
    "        jobs_train = pd.get_dummies(df.job, drop_first=True)\n",
    "\n",
    "        genders_train = pd.get_dummies(df.gender, drop_first=True)\n",
    "\n",
    "        # (CONTINUOUS)\n",
    "        ages_train = 2 * ((df.age - df.age.min()) /\n",
    "                          (df.age.max() - df.age.min())) - 1\n",
    "        return jobs_train.apply(func2), genders_train.apply(func2), ages_train.apply(func1)\n",
    "\n",
    "    @staticmethod\n",
    "    def obfuscation_functions(eps_hat: int = 4):\n",
    "        n_features = 3  # d\n",
    "\n",
    "        delta = n_features + n_features ** 2 / 4  # global sensitivity\n",
    "        # slack = np.random.uniform(0, 1, 1)\n",
    "        # slack = np.max(1, n_features, np.int8(local_epsilon / 2.5))\n",
    "\n",
    "        C = (np.exp(eps_hat / 2) + 1) / (np.exp(eps_hat / 2) - 1)\n",
    "\n",
    "        l = lambda x: ((C + 1) / 2) * x - ((C - 1) / 2)\n",
    "        pi = lambda x: l(x) + C - 1\n",
    "\n",
    "        dfrac = np.exp(eps_hat / 2) / (np.exp(eps_hat / 2) + 1)\n",
    "        const = lambda x: 1 / (np.exp(eps_hat / x) + 1)\n",
    "\n",
    "        def func1(x):\n",
    "            if np.random.uniform(0, 1, 1) < dfrac:\n",
    "                return np.random.uniform(l(x), pi(x), 1)\n",
    "            else:\n",
    "                return np.random.choice(\n",
    "                    [np.random.uniform(-C, l(x), 1),\n",
    "                     np.random.uniform(pi(x), C, 1).squeeze(0)], 1\n",
    "                )\n",
    "        def func2(x):\n",
    "            array = []\n",
    "            for i in x:\n",
    "                if i == 1:\n",
    "                    array.append(np.float32(.5))\n",
    "                else:\n",
    "                    array.append(const(np.random.uniform(0, 1, 1)))\n",
    "            return array\n",
    "        return func1, func2\n",
    "\n",
    "    def add_negatives(self, df: pd.DataFrame, item: str = 'mid', items=None, n_samples: int = 4):\n",
    "        if items is None:\n",
    "            items = set(self.df[item].unique())\n",
    "\n",
    "        movies = set(self.df.mid.unique())\n",
    "        scombine = df.groupby('uid')[item].apply(set).reset_index()\n",
    "        mcombine = df.groupby('uid')['mid'].apply(set).reset_index()\n",
    "        scombine['jnegatives'] = scombine[item].apply(lambda x: sample(list(items - x), n_samples))\n",
    "        mcombine['mnegatives'] = mcombine['mid'].apply(lambda x: sample(list(movies - x), n_samples))\n",
    "\n",
    "        s = scombine.apply(lambda x: pd.Series(x.jnegatives, dtype=np.int16), axis=1).stack().reset_index()\n",
    "        m = mcombine.apply(lambda x: pd.Series(x.mnegatives, dtype=np.int16), axis=1).stack().reset_index()\n",
    "        s.rename(columns={'level_0': 'uid', 0: item}, inplace=True)\n",
    "        m.rename(columns={'level_0': 'uid', 0: 'mid'}, inplace=True)\n",
    "        s.drop(['level_1'], axis=1, inplace=True)\n",
    "        m.drop(['level_1'], axis=1, inplace=True)\n",
    "        s['rating'] = np.int8(0)\n",
    "        s.uid = s.uid.astype(np.int16)\n",
    "        s = pd.merge(s, m, on=['uid', 'uid'], how='inner')\n",
    "        complete = pd.concat([df, s]).sort_values(by=['uid', item])\n",
    "        # complete = pd.concat([df, s]).sort_values(by=['uid', item])\n",
    "        return complete.reset_index(drop=True)\n",
    "\n",
    "    #\n",
    "    # def _train_test_split(self):\n",
    "    #     # self.df['latest'] = self.df.groupby(['uid'])['date'].rank(method='first', ascending=False)\n",
    "    #     test_bool = self.df.latest <= 1\n",
    "    #     test = self.df[test_bool]\n",
    "    #     train = self.df[~test_bool]\n",
    "    #     return (train[['uid', 'mid', 'age', 'gender', 'job', 'rating']],\n",
    "    #             test[['uid', 'mid', 'age', 'gender', 'job', 'rating']]\n",
    "    #             )\n",
    "\n",
    "    def _train_test_split(self, train_ratio: float = .8):\n",
    "        msk = np.random.rand(len(self.df)) < train_ratio\n",
    "\n",
    "        train = self.df[msk]\n",
    "        test = self.df[~msk]\n",
    "        return train, test\n",
    "\n",
    "    def _features(self):\n",
    "        df = pd.read_csv('MovieLens/users.dat',\n",
    "                         sep='::',\n",
    "                         header=None,\n",
    "                         names=['uid', 'gender', 'age', 'job', 'zip'],\n",
    "                         engine='python')\n",
    "        df.drop(columns=['uid'], inplace=True)\n",
    "        df.index.rename('uid', inplace=True)\n",
    "        df.gender = pd.get_dummies(df.gender, drop_first=True)  # 0:F, 1:M\n",
    "        df.reset_index(inplace=True)\n",
    "        drop = [0, 10, 13, 19]\n",
    "\n",
    "        clean = df[~df['job'].isin(drop)]\n",
    "\n",
    "        clean['rating'] = 1\n",
    "        clean['uid'] = clean.uid - 1\n",
    "\n",
    "        self.num_jobs = clean.job.nunique()\n",
    "\n",
    "        item_id = clean[['job']].drop_duplicates()\n",
    "        item_id['njob'] = np.arange(self.num_jobs)\n",
    "        clean = pd.merge(clean, item_id, on=['job'], how='left')\n",
    "        clean.job = clean.njob\n",
    "        return clean\n",
    "\n",
    "\n",
    "class DataGenerator(AttributeData):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.targets = TargetData()\n",
    "        # self.full_train = pd.merge(self.targets.training_data, self.df, on=['uid', 'uid'], how='left')\n",
    "        # self.jobs, self.genders, self.ages = self.perturb_input(self.full_train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.jobs.shape[0]  # Length of the data to train on\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ftank\\AppData\\Local\\Temp/ipykernel_27644/2158403297.py:69: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  users.append(LongTensor([u.uid.values]))\n"
     ]
    }
   ],
   "source": [
    "data = AttributeData()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       uid   mid  age  gender  job  rating\n37357  300   218   18       1   10       1\n37358  300  1321   18       1   10       1\n37359  300    58   18       1   10       1\n37360  300   556   18       1   10       1\n37361  300   236   18       1   10       1\n...    ...   ...  ...     ...  ...     ...\n37707  300    26   18       1   10       1\n37708  300   217   18       1   10       1\n37709  300  1017   18       1   10       1\n37710  300   837   18       1   10       1\n37712  300    52   18       1   10       1\n\n[280 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>mid</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>job</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37357</th>\n      <td>300</td>\n      <td>218</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37358</th>\n      <td>300</td>\n      <td>1321</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37359</th>\n      <td>300</td>\n      <td>58</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37360</th>\n      <td>300</td>\n      <td>556</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37361</th>\n      <td>300</td>\n      <td>236</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>37707</th>\n      <td>300</td>\n      <td>26</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37708</th>\n      <td>300</td>\n      <td>217</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37709</th>\n      <td>300</td>\n      <td>1017</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37710</th>\n      <td>300</td>\n      <td>837</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>37712</th>\n      <td>300</td>\n      <td>52</td>\n      <td>18</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>280 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.df\n",
    "data.train[data.train.uid==300]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       uid   mid   age  gender  job  rating\n34624  300  1942  18.0     1.0    4       0\n34625  300  3034  18.0     1.0    4       0\n34626  300  3266  18.0     1.0    4       0\n34627  300  1878  18.0     1.0    4       0\n34628  300  1942  18.0     1.0    7       0\n...    ...   ...   ...     ...  ...     ...\n34915  300  1878  18.0     1.0   12       0\n34916  300  1942  18.0     1.0   14       0\n34917  300  3034  18.0     1.0   14       0\n34918  300  3266  18.0     1.0   14       0\n34919  300  1878  18.0     1.0   14       0\n\n[296 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>mid</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>job</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>34624</th>\n      <td>300</td>\n      <td>1942</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34625</th>\n      <td>300</td>\n      <td>3034</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34626</th>\n      <td>300</td>\n      <td>3266</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34627</th>\n      <td>300</td>\n      <td>1878</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34628</th>\n      <td>300</td>\n      <td>1942</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>34915</th>\n      <td>300</td>\n      <td>1878</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>12</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34916</th>\n      <td>300</td>\n      <td>1942</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34917</th>\n      <td>300</td>\n      <td>3034</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34918</th>\n      <td>300</td>\n      <td>3266</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>14</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34919</th>\n      <td>300</td>\n      <td>1878</td>\n      <td>18.0</td>\n      <td>1.0</td>\n      <td>14</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>296 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.training_data[data.training_data.uid==300]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "num_epochs = 25\n",
    "batch_size = 2048\n",
    "learning_rate = .001\n",
    "top_k = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_ncf(model):\n",
    "    # data.get_train_instances(seed=e)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),  lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "    dataloader = DataLoader(data, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=0)\n",
    "    t1 = time()\n",
    "    it_per_epoch = len(data) / batch_size\n",
    "    for i in range(num_epochs):\n",
    "        model.train()\n",
    "        print(\"Starting epoch \", i + 1)\n",
    "        j = 0\n",
    "        for batch in dataloader:\n",
    "            # u, j, g, a, r = batch\n",
    "            u, r = batch\n",
    "            print(u)\n",
    "            break\n",
    "            # move tensors to cuda\n",
    "            f = f.to(device)\n",
    "            # m = m.to(device)\n",
    "            r = r.to(device)\n",
    "            y_hat = model(f.squeeze(1), r.squeeze(1))\n",
    "            loss = torch.nn.BCELoss()  # (weight=w, reduction=\"mean\")\n",
    "            loss = loss(y_hat, r.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if j % int(1 + it_per_epoch / 10) == 0:\n",
    "                print(\"Progress: \", round(100 * j / it_per_epoch), \"%\")\n",
    "            j+=1\n",
    "\n",
    "        # Epoch metrics\n",
    "        t2 = time()\n",
    "        print(\"Epoch time:\", round(t2 - t1), \"seconds\")\n",
    "        print(\"Loss:\", loss / i)\n",
    "        ncf.eval()\n",
    "\n",
    "\n",
    "        # print('baseline ')\n",
    "        # t1 = time()\n",
    "        # hr, ndcg = evaluate_model(ncf,data.test[['uid', 'mid']].values,10,100, 3416)\n",
    "        # t2 = time()\n",
    "        # print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        # print(f\"HR@{top_k}:{hr[-1]}\")\n",
    "        #\n",
    "        # print(\"(evaluator)...\")\n",
    "        # t1 = time()\n",
    "        # hr, ndcg = evaluator()\n",
    "        # # hr, ndcg = evaluate(model, data.test, top_K)\n",
    "        # t2 = time()\n",
    "        # print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        # print(f\"HR@{top_k}:{hr}\")\n",
    "\n",
    "        print(\"Evaluating (eval_model)...\")\n",
    "        t1 = time()\n",
    "        hr, ndcg = eval_model(model, data)\n",
    "        t2 = time()\n",
    "        print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        print(f\"HR@{top_k}:{hr}\")\n",
    "        #\n",
    "        #\n",
    "        # print(\"Evaluating (tf)...\")\n",
    "        # t1 = time()\n",
    "        # hr, ndcg = tf_eval(1)\n",
    "        # t2 = time()\n",
    "        #\n",
    "        # print(\"Evaluation time:\", round(t2 - t1), \"seconds\")\n",
    "        # print(f\"HR@{top_k}:{np.array(hr).mean()}\")\n",
    "        # new\n",
    "        # HR, NDCG = evaluate_model(model, data, validation=False)\n",
    "        # updated\n",
    "        # hr, ndcg = evaluate_model(model, data.test, top_K, random_samples)\n",
    "        # original\n",
    "        loss = 0\n",
    "        print()\n",
    "\n",
    "    print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/preTrained_NCF'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_27644/2483868930.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mncf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mNCF\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m6040\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3952\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m128\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m128\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m64\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m16\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[0mncf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'models/preTrained_NCF'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\NFPCF\\venv\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    592\u001B[0m         \u001B[0mpickle_load_args\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'encoding'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    593\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 594\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    595\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_is_zipfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    596\u001B[0m             \u001B[1;31m# The zipfile reader is going to advance the current file position.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\NFPCF\\venv\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0m_is_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 230\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    231\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    232\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m'w'\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\NFPCF\\venv\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_opener\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_open_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'models/preTrained_NCF'"
     ]
    }
   ],
   "source": [
    "from models import NCF\n",
    "\n",
    "ncf = NCF(6040, 3952, 128, [128, 64, 32, 16], 1)\n",
    "\n",
    "ncf.load_state_dict(torch.load('models/preTrained_NCF'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ncf(ncf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.targets.training_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.training_data.dropna().info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = data.add_negatives(data.test, item='job', items = set(data.df.job.unique()))\n",
    "x[x.uid==0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_negatives(df: pd.DataFrame, item: str = 'mid', n_samples: int = 4):\n",
    "    movies = set(data.df.mid.unique())\n",
    "    items = set(data.df.mid.unique())\n",
    "    scombine = df.groupby('uid')[item].apply(set).reset_index()\n",
    "    mcombine = df.groupby('uid')['mid'].apply(set).reset_index()\n",
    "    scombine['jnegatives'] = scombine[item].apply(lambda x: sample(list(items - x), n_samples))\n",
    "    mcombine['mnegatives'] = mcombine['mid'].apply(lambda x: sample(list(movies - x), n_samples))\n",
    "\n",
    "    s = scombine.apply(lambda x: pd.Series(x.jnegatives, dtype=np.int16), axis=1).stack().reset_index()\n",
    "    m = mcombine.apply(lambda x: pd.Series(x.mnegatives, dtype=np.int16), axis=1).stack().reset_index()\n",
    "    s.rename(columns={'level_0': 'uid', 0: item}, inplace=True)\n",
    "    m.rename(columns={'level_0': 'uid', 0: 'mid'}, inplace=True)\n",
    "    s.drop(['level_1'], axis=1, inplace=True)\n",
    "    m.drop(['level_1'], axis=1, inplace=True)\n",
    "    s['rating'] = np.int8(0)\n",
    "    s.uid = s.uid.astype(np.int16)\n",
    "    s = pd.merge(s, m, on=['uid', 'uid'], how='inner')\n",
    "    complete = pd.concat([df, s]).sort_values(by=['uid', item])\n",
    "    # complete = pd.concat([df, s]).sort_values(by=['uid', item])\n",
    "    return complete.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = pd.merge(\n",
    "    data._features(),\n",
    "    data.targets.test,\n",
    "    # on=['uid', 'uid'],\n",
    "    how='left'\n",
    ")[['uid', 'mid', 'age', 'gender', 'job', 'rating']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_test = add_negatives(test, item='job', n_samples=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_test['age'] = full_test.groupby('uid')['age'].transform('first')\n",
    "full_test['gender'] = full_test.groupby('uid')['gender'].transform('first')\n",
    "full_test.dropna(inplace=True)\n",
    "full_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_testing(df):\n",
    "    test = df.sort_values(by=['uid', 'rating'], ascending=False)\n",
    "    users, features, outputs = [], [], []\n",
    "    for _, u in test.groupby('uid'):\n",
    "        users.append(LongTensor([u.uid.values]))\n",
    "        features.append(LongTensor([u[['mid', 'age', 'gender', 'job']].values]))\n",
    "        outputs.append(LongTensor([u.rating.values]))\n",
    "    return users, features, outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensors = parse_testing(full_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rank(l, item):\n",
    "    # rank of the test item in the list of negative instances\n",
    "    # returns the number of elements that the test item is bigger than\n",
    "\n",
    "    index = 0\n",
    "    for element in l:\n",
    "        if element > item:\n",
    "            index += 1\n",
    "            return index\n",
    "        index += 1\n",
    "    return index\n",
    "def eval_model(model, data, num_users=6040):\n",
    "    # Evaluates the model and returns HR@10 and NDCG@10\n",
    "    hits = 0\n",
    "    ndcg = 0\n",
    "    for u in range(num_users):\n",
    "        user = data.testing_tensors[0][u].squeeze().to(device)\n",
    "        item = data.testing_tensors[1][u].squeeze().to(device)\n",
    "        y = model(user, item)\n",
    "\n",
    "        y = y.tolist()\n",
    "        y = sum(y, [])\n",
    "        first = y.pop(0)\n",
    "        y.sort()\n",
    "        ranking = rank(y, first)\n",
    "        if ranking > 90:\n",
    "            hits += 1\n",
    "            ndcg += np.log(2) / np.log(len(user) - ranking + 1)\n",
    "\n",
    "    hr = hits / num_users\n",
    "    ndcg = ndcg / num_users\n",
    "    return hr, ndcg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}